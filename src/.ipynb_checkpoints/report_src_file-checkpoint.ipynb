{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94133d7",
   "metadata": {},
   "source": [
    "# DATA1030 Final Project - Henry Pasts\n",
    "\n",
    "## Can you create a profitable and feasible stock market trading strategy from a Machine Learning Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d2fe1",
   "metadata": {},
   "source": [
    "The problem I want to solve is whether we can accurately predict the next day's price of the S&P 500 using current and historical data.\n",
    "\n",
    "The end goal of the project is to build a stock trading simulation on the test data that will buy or sell (depending on current positioning) a stock market index at the next open.<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) It will use the data available up to the current point of evaluation to make predictions regarding the direction and magnitude of the next day's price.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font size=1><a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) This Project uses the S%P 500, which consists of the 500 largest U.S. Based Companies.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56721246",
   "metadata": {},
   "source": [
    "### Target Variable:\n",
    "I have 2 candidates for the target variable and plan to test them separately:\n",
    "1. The Price of the next day (continuous)\n",
    "2. The percent change of the next dayâ€™s price (continuous)\n",
    "\n",
    "Thus, this is a regression problem, as the values of the next day's price and magniture (percentage) are both continues features.\n",
    "\n",
    "### Importance and Interesting Aspects of the Project\n",
    "The CMT Association<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) studies price trends and anylsis using a tool called \"Technical Analysis\"<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2). Mathematicians and other people interested in stocks have formulated indicators that take in a certain period (data from x days ago) of the last day's price, open, low, high, volume, etc, in order to track and hopefully predict future price trends and momentums. The CMT has been criticized for putting too much emphasis on the predictive ability of indicators, which many skeptics have questioned. I think indicators are valuable, but I also agree that more emphasis needs to be put on prediction and testing predicitve power, thus I plan to incorporate machine learning algoritms to find patterns and increase prediction confidence of the technical indicators and to better bound probabilities.\n",
    "\n",
    "<font size=1><a name=\"cite_note-1\"></a>1.[^](#cite_ref-1) \n",
    "    [CMT Website](https://www.cmtassociation.org)\n",
    "</font>\n",
    "\n",
    "<font size=1><a name=\"cite_note-2\"></a>2.[^](#cite_ref-2) \n",
    "    [Description of Technical Analysis](https://www.investopedia.com/terms/t/technicalanalysis.asp)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97b0d6",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Description of Features:\n",
    "\n",
    "<i> Original Features: </i>\n",
    "\n",
    "- <b>Date:</b> The date for the data (Datetime64 Object)\n",
    "\n",
    "\n",
    "\n",
    "- <b>Open:</b> The Open Price (\\\\$) for the date (float64)\n",
    "- <b>High:</b> The High Price (\\\\$) for the date (float64)\n",
    "- <b>Low:</b> The Low Price (\\\\$) for the date (float64)\n",
    "- <b>Close:</b> The Close Price (\\\\$) for the date (float64)\n",
    "\n",
    "\n",
    "\n",
    "- <b>Volume:</b> The total dollar amount ($) traded for the date (int64)\n",
    "\n",
    "\n",
    "<i> Description of Added Features: </i>\n",
    "I added standard tools from technical analysis that pull information from price. This work serves as 'feature engineering' to some extent but more research will be done on engineering features if model performance is not satisfactory. All feature 'windows', i.e. the lookback period were optimized by using the training data and finding the window values that best correlation to future price. Other features, such as the previous returns, are more relevant to percent change and may be dropped if better results come from predicting price and not the percent change of the next day's price.\n",
    "\n",
    "There is a separate Jupyter Notebook titled \"indicator_optimization\" that runs this optimization. \n",
    "\n",
    "<i> Features: </i>\n",
    "\n",
    "<b><u>Momentum Indicators:</u></b>\n",
    "\n",
    "- <b>AwesomeIndicator:</b> Measures market momentum by subtracting a moving average from the central bar points (high + low) / 2. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#momentum-indicators) (float64)\n",
    "- <b>KAMA:</b>  A moving average that accounts for volatility. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.momentum.KAMAIndicator) (float64)\n",
    "- <b>PercentagePriceOscillator:</b> Measures the difference between two moving averages. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.momentum.PercentagePriceOscillator) (float64)\n",
    "- <b>ROC:</b> Calculates the rate of change of price [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.momentum.ROCIndicator) (float64)\n",
    "- <b>RSI:</b> Compares the magnitude of prior gains and losses to measure how fast price is changing [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.momentum.RSIIndicator) (float64)\n",
    "- <b>StochRSI:</b> Modified RSI that is sensitized. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.momentum.StochRSIIndicator) (float64)\n",
    "- <b>StochasticOscillator:</b> Puts the last close in perspective to the high and lows over a period [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.momentum.StochasticOscillator) (float64)\n",
    "- <b>TSI:</b> Measures trend direction and whether there has been increased buying pressure over the last x days. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.momentum.TSIIndicator) (float64)\n",
    "<br></br>\n",
    "\n",
    "<b><u>Volume Indicators:</u></b>\n",
    "\n",
    "- <b>AccDistIndicator:</b> Measures price accumulation in accordance with volume. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.AccDistIndexIndicator) (float64)\n",
    "- <b>ChaikinMoneyFlow:</b> Measures money flow into a security. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.ChaikinMoneyFlowIndicator) (float64)\n",
    "- <b>EaseOfMovement:</b> Compares price change with volume to assess trend [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.EaseOfMovementIndicator) (float64)\n",
    "- <b>ForceIndex:</b> Measures how strong buying power is [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.ForceIndexIndicator) (float64)\n",
    "- <b>MFI:</b> Uses price and volume to measure buying and selling pressures.  [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.MFIIndicator) (float64)\n",
    "- <b>NegativeVolumeIndex:</b> Measures the amount of negative volume. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.NegativeVolumeIndexIndicator) (float64)\n",
    "- <b>OnBalanceVolume:</b> It relates price and volume in the market and is based on total volume. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.OnBalanceVolumeIndicator) (float64)\n",
    "- <b>VolumePriceTrend:</b> Runs on cummulative volume and adds or subtracts volume depending on the return associated with the volume. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.VolumePriceTrendIndicator) (float64)\n",
    "- <b>VolumeWeightedAveragePrice:</b> Weights Volume with Price  [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volume.VolumeWeightedAveragePrice) (float64)\n",
    "<br></br>\n",
    "\n",
    "<b><u>Volatility Indicators:</u></b>\n",
    "\n",
    "- <b>Average True Range:</b> Provides a degree of price volatility with ranges. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volatility.AverageTrueRange) (float64)\n",
    "- <b>Ulcer Index:</b> Measures Price volatility as a measure of price depreciation from a high. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.volatility.UlcerIndex) (float64)\n",
    "<br></br>\n",
    "\n",
    "<b><u>Trend Indicators:</u></b>\n",
    "\n",
    "- <b>ADX:</b> Measures the average direction of previous prices. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.trend.ADXIndicator) (float64)\n",
    "- <b>Aroon:</b> Identifies when trends are going to reverse by looking at days since a high price. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.trend.AroonIndicator) (float64)\n",
    "- <b>CCI:</b> Measures today's price change vs its average price change over a period [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.trend.CCIIndicator) (float64)\n",
    "- <b>DPO:</b> Used to identify cycles in price [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.trend.DPOIndicator) (float64)\n",
    "- <b>EMA:</b> Measures an average of price using weights on more important data. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.trend.EMAIndicator) (float64)\n",
    "- <b>SMA:</b> A Simple Moving Average of Price - Sum(all x prices) / x. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.trend.SMAIndicator) (float64)\n",
    "- <b>WMA:</b> Assigns more weight to more recent data. [Full Description](https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#ta.trend.WMAIndicator) (float64)\n",
    "\n",
    "- <b>Prev Close Return 1 Day:</b> The Close Return from 1 Day ago. (close - close_1_day_ago) / close_1_day_ago (float64)\n",
    "- <b>Prev Close Return 2 Days:</b> The Close Return from 2 Days ago. (float64)\n",
    "- <b>Prev Close Return 3 Days:</b> The Close Return from 3 Days ago. (float64)\n",
    "\n",
    "\n",
    "- <b>Month:</b> Ordinal Feature Reflecting the Month, a number 1 through 12 (int64)\n",
    "<br></br>\n",
    "\n",
    "<i> Target Variables: </i>\n",
    "\n",
    "- <b>Percent Next Day:</b> The Return of the Next Day (next_price - current_price) / current_price (float64)\n",
    "- <b>Price Next Day:</b> The Price of the Next Day (float64)\n",
    "\n",
    "\n",
    "The data was collected using Yahoo Finance's API yfinance<a name=\"cite_ref-3\"></a>[<sup>[3]</sup>](#cite_note-3) (pip install yfinance).\n",
    "\n",
    "It was collected using the following code:\n",
    "\n",
    "import pandas as pd \\\n",
    "import numpy as np \\\n",
    "import yfinance as yf \n",
    "\n",
    "\n",
    "spy_ohlc_df = yf.download('SPY', start='1993-02-01', end='2022-09-30')\n",
    "\n",
    "\n",
    "### Other Relevant Research: \n",
    "\n",
    "I see other research as a good way to see what I can expect my model to be able to accomplish.\n",
    "\n",
    "Firuz Kamalov, Linda Smail, Ikhlaas Gurrib, 2021, used Deep Learning to predict the direction of the next day's price of the S&P 500 with a non-trivial accuracy of 56%.\n",
    "<a name=\"cite_ref-4\"></a>[<sup>[a]</sup>](#cite_note-4)\n",
    "\n",
    "Alex Fuster and Zhichao Zou at Stanford University used ARIMA, Logistic Regression, Gaussian Discriminant Analysis (GDA), Support Vector Machines, and Neural Networks to predict the spread of a stock pair using a co-integration with S&P500 data and were able to predict the direction of spreads between individual stocks in the S&P 500 with an accuracy and precision of 57-58%, as well as a recall of 90%+. <a name=\"cite_ref-5\"></a>[<sup>[b]</sup>](#cite_note-5)\n",
    "\n",
    "Other research not cited here seems to point to a 54-57% ability to predict the next day's price with mediocre precision and high recall. Getting the direction correct is crucial to avoid getting into a market that will be negative the next day. If the model gets the direction right, then the returns of the next day are a second layer of analysis. To start, I will derive the direction from the predicted magnitude of the next day or if the price is predicted higher or lower than the current day.\n",
    "\n",
    "\n",
    "<font size=1><a name=\"cite_note-3\"></a>3.[^](#cite_ref-3) Yahoo!, Y!Finance, and Yahoo! finance are registered trademarks of Yahoo, Inc. yfinance is not affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. You should refer to Yahoo!'s terms of use ([here](https://legal.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.html), [here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and [here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) for details on your rights to use the actual data downloaded. Remember - the Yahoo! finance API is intended for personal use only.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d232a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ta.momentum import AwesomeOscillatorIndicator, KAMAIndicator, \\\n",
    "    PercentagePriceOscillator, ROCIndicator, RSIIndicator, StochRSIIndicator, \\\n",
    "    StochasticOscillator, TSIIndicator\n",
    "\n",
    "from ta.volume import AccDistIndexIndicator, ChaikinMoneyFlowIndicator, \\\n",
    "    EaseOfMovementIndicator, ForceIndexIndicator, MFIIndicator, \\\n",
    "     NegativeVolumeIndexIndicator, OnBalanceVolumeIndicator, \\\n",
    "        VolumePriceTrendIndicator, VolumeWeightedAveragePrice\n",
    "\n",
    "from ta.volatility import AverageTrueRange, UlcerIndex\n",
    "\n",
    "from ta.trend import ADXIndicator, AroonIndicator, CCIIndicator, DPOIndicator, \\\n",
    "    EMAIndicator, SMAIndicator, WMAIndicator\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a21542",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae343b88",
   "metadata": {},
   "source": [
    "## Visualize Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c97ad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 7269 Rows and 39 Cols for SPY\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spy_ohlc_df = pd.read_excel(\"../data/S&P500 Data.xlsx\")\n",
    "\n",
    "# print columns and rows:\n",
    "print(\"\\n\")\n",
    "print(\"There are \" + str(spy_ohlc_df.shape[0]) + \" Rows and \" + str(spy_ohlc_df.shape[1]) \n",
    "      + \" Cols for SPY\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d42d58",
   "metadata": {},
   "source": [
    "![dist](dist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e7baa",
   "metadata": {},
   "source": [
    "The next day returns of the price seem to be normally distributed with long tails in both directions. This is important information to know when predicting the next day's percentage change, as we know the general distribution the returns are coming from and know that it is centered around 0 with a 1.2% standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1638a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats For Next Day Returns:\n",
      "\n",
      "\n",
      "count    7269.000000\n",
      "mean        0.035781\n",
      "std         1.202915\n",
      "min       -10.942374\n",
      "25%        -0.469539\n",
      "50%         0.063151\n",
      "75%         0.598878\n",
      "max        14.519772\n",
      "Name: Future Gain, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "spy_target_var_df = spy_ohlc_df[[\"Dates\", \"Close\", \"Percent Next Day\", \"Price Next Day\",\n",
    "                                 \"Prev Close Return 1 Day\"]].copy()\n",
    "\n",
    "print(\"Summary Stats For Next Day Returns:\")\n",
    "print(\"\\n\")\n",
    "spy_target_var_df[\"Future Gain\"] = spy_target_var_df[\"Percent Next Day\"] * 100\n",
    "print(spy_target_var_df[\"Future Gain\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a6cb5",
   "metadata": {},
   "source": [
    "![dist](frac_pos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f5a4b",
   "metadata": {},
   "source": [
    "Roughly 50% of the target variable points are positive and 50% are negative, with a slight positive bias. This gives us more information on how hard it may be to predict the magnitude of the next day's price and whether that value is positive or negative and what we can expect a model outperformance of simply choosing positive may represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9a114",
   "metadata": {},
   "source": [
    "![boxplot](month_boxplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf4a25",
   "metadata": {},
   "source": [
    "Returns by month vary and some months express heighted variance, thus we add the Month variable to the feature matrix to account for this and ordinally encode it. March and October look to have more tail-heavy events, which might help the model better gauge season effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f522c",
   "metadata": {},
   "source": [
    "![volume](vol.png)\n",
    "![year returns](year_returns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a752551",
   "metadata": {},
   "source": [
    "Initially, I thought that as volume increase, so would volatility, but volatility looks to be more correlated with the state of the economy, such as major economic events such as the Financial Crisis of 2008 or the Covid-19 Pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c25306",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de305d7",
   "metadata": {},
   "source": [
    "My data is not iid, as previous prices are directly related and correlated to previous prices.\n",
    "\n",
    "Since I am working with time-series data, I split my data with a 60-20-20 train-validation-test split with the earliest data in train.\n",
    "\n",
    "I believe the splitting the data in the tradition time-series manner will be useful, given the forward predictions we are trying to make. It makes sense to see if we can create a model that works well on the price period from 1993 to 2010 and can generalize well to the validation and test data from 2010-2022. Additionally, since we have a major stock market event, namely the 2008 financial crisis in the training data and another major event in the test data, namely the 2020 Covid-19 pandemic, we have volatile events in both periods. \n",
    "\n",
    "Splitting the data is important for deployment, as the model will take in today's prices and historical prices to predict tomorrow's price. Since tomorrow is unknown, treating time linearly by training on a period in the past and seeing if it generalizes well to test data that is close to the future is a good plan for the implementation aspects of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8ded7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the DF contain any nan values? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Does the DF contain any nan values?\", spy_ohlc_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e42e7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting:\n",
    "def basic_split(X,y,train_size,val_size,test_size,random_state):\n",
    "    \n",
    "    # test the inputs\n",
    "    if train_size + val_size + test_size != 1 or type(random_state) != int:\n",
    "        if train_size + val_size + test_size != 1 and type(random_state) == int:\n",
    "            raise ValueError(\"Train Size + Val Size + Test Size must equal 1.\")\n",
    "        elif train_size + val_size + test_size == 1 and type(random_state) != int:\n",
    "            raise ValueError(\"Random State must be an integer.\")\n",
    "        else:  # both tests fail:\n",
    "            raise ValueError(\"Train Size + Val Size + Test Size must equal 1 and Random State must be an integer.\")\n",
    "    \n",
    "    # perform basic split\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X, y, train_size=train_size, \n",
    "                                                          test_size=(val_size+test_size), random_state=random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, train_size=(val_size/(val_size + test_size)), \n",
    "                                                    test_size=(test_size/(val_size + test_size)), \n",
    "                                                    random_state=random_state)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5233fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train price Len: 4361\n",
      "Y train price Len: 4361\n",
      "X val price Len: 1454\n",
      "Y val price Len: 1454\n",
      "X test price Len: 1454\n",
      "Y test price Len: 1454\n",
      "\n",
      "\n",
      "X train pct Len: 4361\n",
      "Y train pct Len: 4361\n",
      "X val pct Len: 1454\n",
      "Y val pct Len: 1454\n",
      "X test pct Len: 1454\n",
      "Y test pct Len: 1454\n"
     ]
    }
   ],
   "source": [
    "# Month Ordinal Encoding:\n",
    "enc = OrdinalEncoder()\n",
    "spy_ohlc_df[\"Month\"] = enc.fit_transform(spy_ohlc_df[\"Month\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# note: the above line is done here because graphs require Date to be DateTime\n",
    "\n",
    "# 60-20-20 (train-val-test) for Price:\n",
    "y_price = spy_ohlc_df[\"Price Next Day\"].to_numpy()\n",
    "X_price = spy_ohlc_df.drop(labels=[\"Percent Next Day\", \"Price Next Day\"], axis=1)\n",
    "X_train_price, y_train_price, X_val_price, y_val_price, X_test_price, y_test_price = \\\n",
    "    basic_split(X_price, y_price, 0.6, 0.2, 0.2, 7)\n",
    "print(\"X train price Len:\", len(X_train_price))\n",
    "print(\"Y train price Len:\", len(y_train_price))\n",
    "\n",
    "print(\"X val price Len:\", len(X_val_price))\n",
    "print(\"Y val price Len:\", len(y_val_price))\n",
    "\n",
    "print(\"X test price Len:\", len(X_test_price))\n",
    "print(\"Y test price Len:\", len(y_test_price))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# 60-20-20 (train-val-test) for Percent Change:\n",
    "y_pct = spy_ohlc_df[\"Percent Next Day\"].to_numpy()\n",
    "X_pct = spy_ohlc_df.drop(labels=[\"Percent Next Day\", \"Price Next Day\"], axis=1)\n",
    "X_train_pct, y_train_pct, X_val_pct, y_val_pct, X_test_pct, y_test_pct = \\\n",
    "    basic_split(X_pct, y_pct, 0.6, 0.2, 0.2, 7)\n",
    "print(\"X train pct Len:\", len(X_train_pct))\n",
    "print(\"Y train pct Len:\", len(y_train_pct))\n",
    "\n",
    "print(\"X val pct Len:\", len(X_val_pct))\n",
    "print(\"Y val pct Len:\", len(y_val_pct))\n",
    "\n",
    "print(\"X test pct Len:\", len(X_test_pct))\n",
    "print(\"Y test pct Len:\", len(y_test_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c515966",
   "metadata": {},
   "source": [
    "Now, I will apply a StandardScaler to all continuous columns as well as a separate DataFrame that uses MinMaxScaler and I will compare both in my modeling process. It makes sense to normalize variables related to percent change with a StandardScaler due to the mean and standard deviation normalization. In order to creata a more stable approximation of the next day's price, a MinMaxScaler may be appropriate, so testing both depending on the target variable (next day's price or next day's percent change) is needed.\n",
    "\n",
    "I apply an OrdinalEncoder to the Month variable to convert it to a value (1 to 12).\n",
    "\n",
    "For now, I drop the Date variable, which is of type 'DateTime', but will experiment with including it instead of the month variable as a timestamp number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4486349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Apply Scalers:\n",
    "\n",
    "# StandardScaler on data with target variable of 'Price':\n",
    "standard_scaler_X = StandardScaler()\n",
    "standard_scaler_X.fit(X_train_price)\n",
    "\n",
    "X_train_price = standard_scaler_X.transform(X_train_price)\n",
    "X_val_price = standard_scaler_X.transform(X_val_price)\n",
    "X_test_price = standard_scaler_X.transform(X_test_price)\n",
    "\n",
    "# reshape y:\n",
    "y_train_price = y_train_price.reshape(-1, 1)\n",
    "y_val_price = y_val_price.reshape(-1, 1)\n",
    "y_test_price = y_test_price.reshape(-1, 1)\n",
    "\n",
    "standard_scaler_y = StandardScaler()\n",
    "standard_scaler_y.fit(y_train_price)\n",
    "\n",
    "y_train_price = standard_scaler_y.transform(y_train_price)\n",
    "y_val_price = standard_scaler_y.transform(y_val_price)\n",
    "y_test_price = standard_scaler_y.transform(y_test_price)\n",
    "\n",
    "\n",
    "# MinMaxScaler on data with target variable of 'Percent Change':\n",
    "min_max_scaler_X = MinMaxScaler()\n",
    "min_max_scaler_X.fit(X_train_pct)\n",
    "\n",
    "X_train_pct = min_max_scaler_X.transform(X_train_pct)\n",
    "X_val_pct = min_max_scaler_X.transform(X_val_pct)\n",
    "X_test_pct = min_max_scaler_X.transform(X_test_pct)\n",
    "\n",
    "# reshape y:\n",
    "y_train_pct = y_train_pct.reshape(-1, 1)\n",
    "y_val_pct = y_val_pct.reshape(-1, 1)\n",
    "y_test_pct = y_test_pct.reshape(-1, 1)\n",
    "\n",
    "min_max_scaler_y = MinMaxScaler()\n",
    "min_max_scaler_y .fit(y_train_pct)\n",
    "\n",
    "y_train_pct = min_max_scaler_y.transform(y_train_pct)\n",
    "y_val_pct = min_max_scaler_y.transform(y_val_pct)\n",
    "y_test_pct = min_max_scaler_y.transform(y_test_pct)\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c4ac2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 39 Preprocessed Features in the Data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print columns and rows after Preprocessing:\n",
    "print(\"\\n\")\n",
    "print(\"There are \" + str(spy_ohlc_df.shape[1]) + \" Preprocessed Features in the Data\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81e6dc53",
   "metadata": {},
   "source": [
    "# Github Repo\n",
    "\n",
    "https://github.com/henrypasts/data1030_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ee0c3",
   "metadata": {},
   "source": [
    "# References\n",
    "<font size=4><a name=\"cite_note-4\"></a>a.[^](#cite_ref-4)\n",
    "Kamaloc, Firuz, Smail, Linda, Gurrib, Ikhlaas (2021). FORECASTING WITH DEEP LEARNING: S&P 500 INDEX </font>\n",
    "\n",
    "<font size=4><a name=\"cite_note-5\"></a>b.[^](#cite_ref-5)\n",
    "Fuster, Alex, Zou, Zhichao (2018). Using Machine Learning Models to Predict S&P500 Price\n",
    "Level and Spread Direction </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
